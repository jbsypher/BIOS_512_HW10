{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d684172-4b94-42cf-bda2-e11952420d86",
   "metadata": {},
   "source": [
    "# Homework 10\n",
    "#### Course Notes\n",
    "**Language Models:** https://github.com/rjenki/BIOS512/tree/main/lecture17  \n",
    "**Unix:** https://github.com/rjenki/BIOS512/tree/main/lecture18  \n",
    "**Docker:** https://github.com/rjenki/BIOS512/tree/main/lecture19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d839a5ba-62f4-4699-baea-018afda70786",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "#### Make a language model that uses ngrams and allows the user to specify start words, but uses a random start if one is not specified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef37d3a-a6ad-42ae-9e16-7d7338c9ce49",
   "metadata": {},
   "source": [
    "#### a) Make a function to tokenize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82e2051f-01a3-438f-8fbe-d6f3d972850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tokenizers)\n",
    "\n",
    "tokenize_text <- function(text) {\n",
    "  tokenizers::tokenize_words(text, lowercase=TRUE, strip_punct=TRUE)[[1]]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86145513-294b-4894-a02c-8ae60e2c616e",
   "metadata": {},
   "source": [
    "#### b) Make a function generate keys for ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2350a0e-ad76-4855-8559-5e496f09b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_from <- function(ngram, sep = \"\\x1f\") {\n",
    "  paste(ngram, collapse=sep)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52988c2c-b230-467f-b519-72bc85b93b43",
   "metadata": {},
   "source": [
    "#### c) Make a function to build an ngram table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "080a494a-fb9e-40c4-bc4f-ff2d56fc3e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_ngram_table <- function(tokens, n, sep = \"\\x1f\") {\n",
    "  if (length(tokens) < n) return(new.env(parent = emptyenv()))\n",
    "  \n",
    "  tbl <- new.env(parent = emptyenv())\n",
    "  \n",
    "  for (i in seq_len(length(tokens) - n + 1L)) {\n",
    "    ngram <- tokens[i:(i + n - 2L)]\n",
    "    \n",
    "    next_word <- tokens[i + n - 1L]\n",
    "    \n",
    "    key <- paste(ngram, collapse = sep)\n",
    "    \n",
    "    counts <- if (!is.null(tbl[[key]])) tbl[[key]] else integer(0)\n",
    "    \n",
    "    if (next_word %in% names(counts)) {\n",
    "      counts[[next_word]] <- counts[[next_word]] + 1L\n",
    "    } else {\n",
    "      counts[[next_word]] <- 1L\n",
    "    }\n",
    "    \n",
    "    tbl[[key]] <- counts\n",
    "  }\n",
    "  tbl\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca6db37-abce-4705-9784-e1b898174f00",
   "metadata": {},
   "source": [
    "#### d) Function to digest the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0fd03555-3b4f-493e-a799-c9fea2f9d00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "digest_text <- function(text, n) {\n",
    "  tokens <- tokenize_text(text)\n",
    "  build_ngram_table(tokens, n)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fff313-0f13-479b-94df-7588c19fdd3d",
   "metadata": {},
   "source": [
    "#### e) Function to digest the url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f18e5e3e-1db8-44df-b8fa-af3b5ff5ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "digest_url <- function(url, n) {\n",
    "  res <- httr::GET(url)\n",
    "  \n",
    "  txt <- httr::content(res, as = \"text\", encoding = \"UTF-8\")\n",
    "  \n",
    "  digest_text(txt, n)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4aa4e73-ee6f-4569-9a54-9d7f7eb3f80a",
   "metadata": {},
   "source": [
    "#### f) Function that gives random start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27051d9e-f9c8-4087-b4ba-eb650f32ee6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_start <- function(tbl, sep = \"\\x1f\") {\n",
    "  keys <- ls(envir = tbl, all.names=TRUE)\n",
    "  \n",
    "  if (length(keys) == 0) stop(\"No n-grams available. Digest text first.\")\n",
    "  \n",
    "  picked <- sample(keys, 1)\n",
    "  \n",
    "  strsplit(picked, sep, fixed=TRUE)[[1]]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e998fb24-f2d6-41bc-a751-1f6accd3411f",
   "metadata": {},
   "source": [
    "#### g) Function to predict the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78d49491-9feb-41d1-8bf8-758982a26b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_next_word <- function(tbl, ngram, sep = \"\\x1f\") {\n",
    "  key <- paste(ngram, collapse = sep)\n",
    "  \n",
    "  counts <- if(!is.null(tbl[[key]])) tbl[[key]] else integer(0)\n",
    "  \n",
    "  if (length(counts) == 0) return(NA_character_)\n",
    "  \n",
    "  sample(names(counts), size=1, prob=as.numeric(counts))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f4002-4932-42c4-a4af-8689293a5857",
   "metadata": {},
   "source": [
    "#### h) Function that puts everything together. Specify that if the user does not give a start word, then the random start will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "832b2e96-f51d-44bb-9148-0354e47a38bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_ngram_generator <- function(tbl, n, sep = \"\\x1f\") {\n",
    "  force(tbl); n <- as.integer(n); force(sep)\n",
    "  \n",
    "  function(start_words = NULL, length = 10L) {\n",
    "    \n",
    "    if ((is.null(start_words)) || length(start_words) != n - 1L) {\n",
    "      start_words <- random_start(tbl, sep=sep)\n",
    "    }\n",
    "    \n",
    "    word_sequence <- start_words\n",
    "    \n",
    "    for (i in seq_len(max(0L, length - length(start_words)))) {\n",
    "      \n",
    "      ngram <- tail(word_sequence, n - 1L)\n",
    "      \n",
    "      next_word <- predict_next_word(tbl, ngram, sep=sep)\n",
    "      \n",
    "      if (is.na(next_word)) break\n",
    "      \n",
    "      word_sequence <- c(word_sequence, next_word)\n",
    "    }\n",
    "    \n",
    "    paste(word_sequence, collapse= \" \")\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b742c67-907c-4bc7-8df1-c84fa65a7554",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "#### For this question, set `seed=2025`.\n",
    "#### a) Test your model using a text file of [Grimm's Fairy Tails](https://www.gutenberg.org/cache/epub/2591/pg2591.txt)\n",
    "#### i) Using n=3, with the start word(s) \"the king\", with length=15. \n",
    "#### ii) Using n=3, with no start word, with length=15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1b2aaf2c-7c1b-492b-9ffb-2340c9fc8386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task (i) Output:\n",
      "[1] \"the king has forbidden me to marry another husband am not i shall ride upon\"\n",
      "\n",
      "Task (ii) Output:\n",
      "[1] \"song was over the lake and herself into her little daughterâ€™s hand and was about\"\n"
     ]
    }
   ],
   "source": [
    "set.seed(2025)\n",
    "\n",
    "file_content <- readLines(\"Grimm.txt\", warn = FALSE)\n",
    "full_text <- paste(file_content, collapse = \"\\n\")\n",
    "\n",
    "ngram_table <- digest_text(full_text, n = 3)\n",
    "\n",
    "generate_grimm <- make_ngram_generator(ngram_table, n = 3)\n",
    "\n",
    "cat(\"Task (i) Output:\\n\")\n",
    "output_i <- generate_grimm(start_words = c(\"the\", \"king\"), length = 15)\n",
    "print(output_i)\n",
    "\n",
    "cat(\"\\nTask (ii) Output:\\n\")\n",
    "output_ii <- generate_grimm(length = 15)\n",
    "print(output_ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e04b167-7f2c-4e0f-88e7-86ba5e8d74cc",
   "metadata": {},
   "source": [
    "#### b) Test your model using a text file of [Ancient Armour and Weapons in Europe](https://www.gutenberg.org/cache/epub/46342/pg46342.txt)\n",
    "#### i) Using n=3, with the start word(s) \"the king\", with length=15. \n",
    "#### ii) Using n=3, with no start word, with length=15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a5df3b8-048b-482e-beb8-21217e26a710",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task (i) Output (Start: 'the king'):\n",
      "[1] \"the king he added to the entire exclusion of the swords were made prisoners the\"\n",
      "\n",
      "Task (ii) Output (Random Start):\n",
      "[1] \"king was campaigning in france denmark germany switzerland and livonia figures 5 and the sword\"\n"
     ]
    }
   ],
   "source": [
    "set.seed(2025)\n",
    "\n",
    "file_content <- readLines(\"ArmourEurope.txt\", warn = FALSE)\n",
    "full_text <- paste(file_content, collapse = \"\\n\")\n",
    "\n",
    "ngram_table <- digest_text(full_text, n = 3)\n",
    "\n",
    "generate_armour <- make_ngram_generator(ngram_table, n = 3)\n",
    "\n",
    "\n",
    "cat(\"Task (i) Output (Start: 'the king'):\\n\")\n",
    "output_i <- generate_armour(start_words = c(\"the\", \"king\"), length = 15)\n",
    "print(output_i)\n",
    "\n",
    "\n",
    "cat(\"\\nTask (ii) Output (Random Start):\\n\")\n",
    "output_ii <- generate_armour(length = 15)\n",
    "print(output_ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fb37ad-8e7c-4e62-afc0-ba46d46401fc",
   "metadata": {},
   "source": [
    "#### c) Explain in 1-2 sentences the difference in content generated from each source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256cb061-38ae-4f87-9ab4-3c3f5572084e",
   "metadata": {},
   "source": [
    "The content from source #1 was a story and therefore the output was very conversational about marrige and family."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541a1d8c-ea42-44bd-b530-eb4a329b81c7",
   "metadata": {},
   "source": [
    "The content from source #2 however was much more academic and factual in nature, which caused the output to be more factual by listing the geographies the king visited and listing a reference to \"figure 5.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e45972-f441-4d07-9073-fcddd6146cbd",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "#### a) What is a language learning model? \n",
    "#### b) Imagine the internet goes down and you can't run to your favorite language model for help. How do you run one locally?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c88a4b2-6a9d-4ad0-84f8-2d411216c006",
   "metadata": {},
   "source": [
    "A.)\n",
    "\n",
    "At the simplest level, a language learning model is a fancy autocomplete that uses AI to find complex patterns in text. Therefore, it focuses on the most common answers, and as it is trained on human data, it can often be incorrect. It uses the probability of the next word to predict what will come next in the sequence.\n",
    "\n",
    "B.)\n",
    "\n",
    "You can use OLLAMA (which is a wrapper around docker) to run a LLM locally. You are basically downloading the learned patterns that the LLM developed from the huge amounts of training data.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a743b-f814-4a53-96e6-8bccb3d34ab8",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "#### Explain what the following vocab words mean in the context of typing `mkdir project` into the command line. If the term doesn't apply to this command, give the definition and/or an example.\n",
    "| Term | Meaning |  \n",
    "|------|---------|\n",
    "| **Shell** | The main program that reads all inputs on the command line and translates it into actions. |\n",
    "| **Terminal emulator** | It's the window (Terminal on Mac) that has the UI that you see and and shows you the text you type and the stuff you enter into the shell. |\n",
    "| **Process** | This is the job or process that happens as a result of your shell command input. |\n",
    "| **Signal** | This isn't involved with mkdir project. A signal is something sent to a process. For example, telling a process to stop. |\n",
    "| **Standard input** | This isn't involved with mkdir project. This happens when you use the pipe operator and is the source for command that requires an input. |\n",
    "| **Standard output** | Nothing is written to standard output with the mkdir project command. This usually writes results or messages on terminal screen. |\n",
    "| **Command line argument** | \"project\" is a command line argument. It's additional info that is added to a command. |\n",
    "| **The environment** | This is the background info that the shell passes to the mkdir job. It shows things like the path. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1332ff27-ca3f-4f7e-b4b9-07ead0358dd2",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "#### Consider the following command `find . -iname \"*.R\" | xargs grep read_csv`.\n",
    "#### a) What are the programs?\n",
    "#### b) Explain what this command is doing, part by part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c47d88-c33d-4de2-8c1e-da992b105ef0",
   "metadata": {},
   "source": [
    "a.) \n",
    "The program is organized as such: \n",
    "\n",
    "    1.)the find\n",
    "    \n",
    "    2.)the grep (using uses the results of the find as input)\n",
    "\n",
    "\n",
    "b.) \n",
    "    \"find\" starts to search files\n",
    "    \n",
    "    \".\" searches the current working directory\n",
    "    \n",
    "    \"-iname\"*.R\" is the search criteria, basically looking for files that end with .R (ignoring case because of the -iname flag)\n",
    "    \n",
    "    \n",
    "    | is a pipe to the second command\n",
    "    \n",
    "    \"xargs\" takes the input generaded by find and uses it as input for grep\n",
    "    \n",
    "    \"grep read_csv\" then searches each file it was given (given by xargs) for the text pattern read_csv.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69771ac7-865e-4d82-aa25-a39e7c1ab095",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "#### Install Docker on your machine. See [here](https://github.com/rjenki/BIOS512/blob/main/lecture18/docker_install.md) for instructions. \n",
    "#### a) Show the response when you run `docker run hello-world`.\n",
    "#### b) Access Rstudio through a Docker container. Set your password and make sure your files show up on the Rstudio server. Type the command and the output you get below.\n",
    "#### c) How do you log in to the RStudio server?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d76b7d3-9cf1-4177-ae40-d3c20eba001a",
   "metadata": {},
   "source": [
    "A.) \n",
    "\n",
    "mac:~ $ docker run hello-world\n",
    "Unable to find image 'hello-world:latest' locally\n",
    "latest: Pulling from library/hello-world\n",
    "198f93fd5094: Pull complete \n",
    "Digest: sha256:f7931603f70e13dbd844253370742c4fc4202d290c80442b2e68706d8f33ce26\n",
    "Status: Downloaded newer image for hello-world:latest\n",
    "\n",
    "Hello from Docker!\n",
    "This message shows that your installation appears to be working correctly.\n",
    "\n",
    "To generate this message, Docker took the following steps:\n",
    " 1. The Docker client contacted the Docker daemon.\n",
    " 2. The Docker daemon pulled the \"hello-world\" image from the Docker Hub.\n",
    "    (arm64v8)\n",
    " 3. The Docker daemon created a new container from that image which runs the\n",
    "    executable that produces the output you are currently reading.\n",
    " 4. The Docker daemon streamed that output to the Docker client, which sent it\n",
    "    to your terminal.\n",
    "\n",
    "To try something more ambitious, you can run an Ubuntu container with:\n",
    " $ docker run -it ubuntu bash\n",
    "\n",
    "Share images, automate workflows, and more with a free Docker ID:\n",
    " https://hub.docker.com/\n",
    "\n",
    "For more examples and ideas, visit:\n",
    " https://docs.docker.com/get-started/\n",
    "\n",
    "mac:~ $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b1ecac-932a-4543-8026-42bb2ec0d5cc",
   "metadata": {},
   "source": [
    "B.)\n",
    "\n",
    "mac:jonathanbarta $ docker run --platform linux/amd64 -it -p 8787:8787 -e PASSWORD=password -v /Users/jonathanbarta/code/BIOS_512/BIOS_512_HW10:/home/rstudio/work rocker/verse\n",
    "Unable to find image 'rocker/verse:latest' locally\n",
    "latest: Pulling from rocker/verse\n",
    "4b3ffd8ccb52: Pull complete \n",
    "2c9ba66d5dbe: Pull complete \n",
    "b71e78fefbbb: Pull complete \n",
    "2a63ed8b2250: Pull complete \n",
    "999e4b8f7ed8: Pull complete \n",
    "3c7cdccc4be7: Pull complete \n",
    "04c61279cc76: Pull complete \n",
    "7da3fea5923e: Pull complete \n",
    "7bca23a8b40d: Pull complete \n",
    "e82dc96b20d6: Pull complete \n",
    "7f54ce591537: Pull complete \n",
    "53593fccee71: Pull complete \n",
    "255aa55589e3: Pull complete \n",
    "983a57e0f10d: Pull complete \n",
    "7acb5d2ece3f: Pull complete \n",
    "fc14ca29bd0e: Pull complete \n",
    "3deebd4cc2ea: Pull complete \n",
    "bcdf914130e3: Pull complete \n",
    "339259f92146: Pull complete \n",
    "12b920580d3a: Pull complete \n",
    "33aa1b89cc9c: Pull complete \n",
    "a7519eda3916: Pull complete \n",
    "b615453605c4: Pull complete \n",
    "Digest: sha256:96e1068eed2400e24c337a7ab53c7aab136970d92c1612bb3a1bb0c8972c7bf4\n",
    "Status: Downloaded newer image for rocker/verse:latest\n",
    "[s6-init] making user provided files available at /var/run/s6/etc...exited 0.\n",
    "[s6-init] ensuring user provided files have correct perms...exited 0.\n",
    "[fix-attrs.d] applying ownership & permissions fixes...\n",
    "[fix-attrs.d] done.\n",
    "[cont-init.d] executing container initialization scripts...\n",
    "[cont-init.d] 01_set_env: executing... \n",
    "skipping /var/run/s6/container_environment/HOME\n",
    "skipping /var/run/s6/container_environment/PASSWORD\n",
    "skipping /var/run/s6/container_environment/RSTUDIO_VERSION\n",
    "[cont-init.d] 01_set_env: exited 0.\n",
    "[cont-init.d] 02_userconf: executing... \n",
    "[cont-init.d] 02_userconf: exited 0.\n",
    "[cont-init.d] done.\n",
    "[services.d] starting services\n",
    "[services.d] done.\n",
    "TTY detected. Printing informational message about logging configuration. Logging configuration loaded from '/etc/rstudio/logging.conf'. Logging to 'syslog'.\n",
    "TTY detected. Printing informational message about logging configuration. Logging configuration loaded from '/etc/rstudio/logging.conf'. Logging to 'syslog'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80cbbe1-f1bd-4598-94dd-432eb36c2664",
   "metadata": {},
   "source": [
    "C.)\n",
    "\n",
    "I opened my browser to http://localhost:8787, logged in with username 'rstudio' and password 'password', and confirmed my local files were visible in the 'work' directory in the Files pane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a9efe3-08cf-47cf-9e42-d58c5b55b4df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
